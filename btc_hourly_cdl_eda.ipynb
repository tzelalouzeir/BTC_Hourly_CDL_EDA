{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ylYKJu8GWNld"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.upload()\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!pip install kaggle\n",
        "!kaggle datasets download -d mczielinski/bitcoin-historical-data\n",
        "!unzip bitcoin-historical-data.zip\n",
        "!pip install ccxt\n",
        "!curl -L http://prdownloads.sourceforge.net/ta-lib/ta-lib-0.4.0-src.tar.gz -O && tar xzvf ta-lib-0.4.0-src.tar.gz\n",
        "!cd ta-lib && ./configure --prefix=/usr && make && make install && cd - && pip install ta-lib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import talib\n",
        "import numpy as np\n",
        "\n",
        "# Load data\n",
        "data = pd.read_csv('/content/bitstampUSD_1-min_data_2012-01-01_to_2021-03-31.csv')\n",
        "\n",
        "# Convert the timestamp to datetime and set as index\n",
        "data['Timestamp'] = pd.to_datetime(data['Timestamp'], unit='s')\n",
        "data.set_index('Timestamp', inplace=True)\n",
        "\n",
        "# Fill any missing values forward then backward\n",
        "data.fillna(method='ffill', inplace=True)\n",
        "data.fillna(method='bfill', inplace=True)\n",
        "\n",
        "# Resample to hourly timeframe for this analysis\n",
        "data = data.resample('H').agg({\n",
        "    'Open': 'first',\n",
        "    'High': 'max',\n",
        "    'Low': 'min',\n",
        "    'Close': 'last',\n",
        "    'Volume_(BTC)': 'sum',\n",
        "    'Volume_(Currency)': 'sum'\n",
        "})\n",
        "\n",
        "# Calculate necessary indicators\n",
        "data['Next_Close_Change'] = data['Close'].pct_change().shift(-1)\n",
        "data['SMA50'] = talib.SMA(data['Close'], timeperiod=50)\n",
        "data['SMA200'] = talib.SMA(data['Close'], timeperiod=200)\n",
        "data['ATR'] = talib.ATR(data['High'], data['Low'], data['Close'], timeperiod=14)\n",
        "data['Volume_MA_20'] = data['Volume_(BTC)'].rolling(window=20).mean()\n",
        "data['Candle_Size'] = data['High'] - data['Low']\n",
        "data['Prev_High'] = data['High'].shift(1)\n",
        "data['Prev_Low'] = data['Low'].shift(1)\n",
        "\n",
        "# Daily resample to calculate daily trends\n",
        "data_daily = data.resample('D').agg({\n",
        "    'Open': 'first',\n",
        "    'High': 'max',\n",
        "    'Low': 'min',\n",
        "    'Close': 'last',\n",
        "    'Volume_(BTC)': 'sum',\n",
        "    'Volume_(Currency)': 'sum'\n",
        "})\n",
        "data_daily['SMA_20'] = talib.SMA(data_daily['Close'], timeperiod=20)\n",
        "data['Daily_Trend'] = data_daily['SMA_20'].reindex(data.index, method='ffill')\n",
        "\n",
        "# Trend Identification\n",
        "data['Trend'] = np.where(data['SMA50'] > data['SMA200'], 'Uptrend', 'Downtrend') # Golden , Death\n",
        "data['Trend'] = np.where(\n",
        "    (data['Close'] <= data['SMA50'] * 1.02) & (data['Close'] >= data['SMA50'] * 0.98),\n",
        "    'Sideways',\n",
        "    data['Trend']\n",
        ")\n"
      ],
      "metadata": {
        "id": "Zs8i_1WZWQh6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate bullish or bearish performance periods\n",
        "def calculate_performance_periods(data, pattern_occurrences, start_period, end_period):\n",
        "    bullish_counts = []\n",
        "    bearish_counts = []\n",
        "\n",
        "    for index in pattern_occurrences.index:\n",
        "        bull_count = 0\n",
        "        bear_count = 0\n",
        "\n",
        "        for i in range(start_period, end_period + 1):\n",
        "            if index + pd.Timedelta(hours=i) in data.index:\n",
        "                if data.loc[index + pd.Timedelta(hours=i), 'Close'] > data.loc[index + pd.Timedelta(hours=i-1), 'Close']:\n",
        "                    bull_count += 1\n",
        "                elif data.loc[index + pd.Timedelta(hours=i), 'Close'] < data.loc[index + pd.Timedelta(hours=i-1), 'Close']:\n",
        "                    bear_count += 1\n",
        "\n",
        "        bullish_counts.append(bull_count)\n",
        "        bearish_counts.append(bear_count)\n",
        "\n",
        "    return np.mean(bullish_counts), np.mean(bearish_counts)\n",
        "\n",
        "# Initialize detailed analysis list\n",
        "detailed_analysis = []\n",
        "\n",
        "# Get all candlestick pattern functions from TA-Lib\n",
        "pattern_functions = [func for func in dir(talib) if func.startswith('CDL')]\n",
        "\n",
        "# Loop through all candlestick patterns\n",
        "for pattern in pattern_functions:\n",
        "    pattern_function = getattr(talib, pattern)\n",
        "    data[pattern] = pattern_function(data['Open'], data['High'], data['Low'], data['Close'])\n",
        "\n",
        "    # Identify where the pattern occurs\n",
        "    pattern_occurrences = data[data[pattern] != 0]\n",
        "\n",
        "    if not pattern_occurrences.empty:\n",
        "        # Immediate effects (1-7 hours)\n",
        "        immediate_bullish, immediate_bearish = calculate_performance_periods(data, pattern_occurrences, start_period=1, end_period=7)\n",
        "\n",
        "        # Long-term effects (8-24 hours)\n",
        "        longer_term_bullish, longer_term_bearish = calculate_performance_periods(data, pattern_occurrences, start_period=8, end_period=24)\n",
        "\n",
        "        # Additional features\n",
        "        #pattern_occurrences['Volatility'] = pattern_occurrences['ATR']\n",
        "        pattern_occurrences['Volume_Condition'] = np.where(pattern_occurrences['Volume_(BTC)'] > pattern_occurrences['Volume_MA_20'], 'High Volume', 'Low Volume')\n",
        "        pattern_occurrences['Candle_Size_Relative'] = pattern_occurrences['Candle_Size'] / pattern_occurrences['ATR']\n",
        "        pattern_occurrences['Support_Resistance'] = np.where(\n",
        "            (pattern_occurrences['Close'] > pattern_occurrences['Prev_High']) | (pattern_occurrences['Close'] < pattern_occurrences['Prev_Low']),\n",
        "            'Breach', 'No Breach'\n",
        "        )\n",
        "        pattern_occurrences['Daily_Trend_Alignment'] = np.where(pattern_occurrences['Close'] > pattern_occurrences['Daily_Trend'], 'Uptrend', 'Downtrend')\n",
        "\n",
        "        # Summarize analysis for this pattern\n",
        "        analysis_summary = {\n",
        "            'Pattern': pattern,\n",
        "            'Occurrences': len(pattern_occurrences),\n",
        "            'Avg_Next_Close_Change': pattern_occurrences['Next_Close_Change'].mean(),\n",
        "            'Bullish_Percentage': (pattern_occurrences['Next_Close_Change'] > 0).mean() * 100,\n",
        "            'Bearish_Percentage': (pattern_occurrences['Next_Close_Change'] < 0).mean() * 100,\n",
        "            'Avg_Immediate_Bullish_Hours': immediate_bullish,\n",
        "            'Avg_Immediate_Bearish_Hours': immediate_bearish,\n",
        "            'Avg_Long_Term_Bullish_Hours': longer_term_bullish,\n",
        "            'Avg_Long_Term_Bearish_Hours': longer_term_bearish,\n",
        "            'Uptrend_Percentage': (pattern_occurrences['Trend'] == 'Uptrend').mean() * 100,\n",
        "            'Downtrend_Percentage': (pattern_occurrences['Trend'] == 'Downtrend').mean() * 100,\n",
        "            'Sideways_Percentage': (pattern_occurrences['Trend'] == 'Sideways').mean() * 100,\n",
        "            'High_Volume_Percentage': (pattern_occurrences['Volume_Condition'] == 'High Volume').mean() * 100,\n",
        "            'Support_Resistance_Breach_Percentage': (pattern_occurrences['Support_Resistance'] == 'Breach').mean() * 100,\n",
        "            'Daily_Trend_Alignment_Percentage': (pattern_occurrences['Daily_Trend_Alignment'] == pattern_occurrences['Trend']).mean() * 100,\n",
        "            'Best_Hour_of_Day': pattern_occurrences.groupby(pattern_occurrences.index.hour)['Next_Close_Change'].mean().idxmax()\n",
        "        }\n",
        "\n",
        "        # Append the summary to the detailed analysis list\n",
        "        detailed_analysis.append(analysis_summary)\n",
        "\n",
        "# Convert the detailed analysis list to a DataFrame for better readability\n",
        "detailed_analysis_df = pd.DataFrame(detailed_analysis)\n",
        "\n",
        "# Display the detailed analysis DataFrame\n",
        "print(detailed_analysis_df)\n",
        "\n",
        "# Save to CSV if needed\n",
        "detailed_analysis_df.to_csv('kaggle_hourly_pattern_analysis.csv', index=False)"
      ],
      "metadata": {
        "id": "fCKzN9pcWR0d"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}